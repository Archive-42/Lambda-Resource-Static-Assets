<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>README</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
</head>
<body>
<blockquote>
<p>:warning: <strong>This is an experimental feature</strong></p>
</blockquote>
<h1 id="static-runtime">Static Runtime</h1>
<p>The premise of this approach is that a small subset of neural networks are well represented by a completely flattened dataflow graph. TorchScript supports a far more feature programming paradigm, so many models will not work out of the box.</p>
<h2 id="assumptions">Assumptions</h2>
<p>This is a list of current assumptions for use with this feature.</p>
<ul>
<li>Inference only execution</li>
</ul>
<p>After <code>torch.jit.freeze</code> and inlining/constant propagation is run on the model:</p>
<ul>
<li>No control flow</li>
<li>No submodule invocations</li>
<li>No references to <code>self</code></li>
<li>Inlined weights (i.e. no calls to <code>GetAttr</code>)</li>
</ul>
<h2 id="threading-model">Threading model</h2>
<p>Static runtime supports two execution modes.</p>
<p>Mode 1: single-threaded with no parallelism except for intra-op parallelism. For this mode, you can do either:</p>
<pre><code>  // m is the TorchScript module
  auto runtime = StaticRuntime(m, opts);
  auto output = runtime.run(args, kwargs);</code></pre>
<p>or</p>
<pre><code>  auto mod = PrepareForStaticRuntime(m);
  auto runtime = StaticRuntime(mod, opts);
  auto output = runtime.run(args, kwargs);</code></pre>
<p>Mode 2: similar to data parallelism, run the same model for different inputs on different threads at the same time. In this case, run <code>PrepareForStaticRuntime</code> to prepare the graph for Static Runtime. You should have one InferenceModule instance per model, and one Static Runtime instance per running thread. To avoiding creating StaticRuntime on the fly, use a synchronized stack (i.e. <code>boost::lockfree::stack</code>) to cache all the Static Runtime instances in your code.</p>
<pre><code>  // initialization
  auto mod = PrepareForStaticRuntime(m);
  // 128 is good for most cases. Pick a number that works for you
  boost::lockfree::stack&lt;std::shared_ptr&lt;StaticRuntime&gt;,
    boost::lockfree::fixed_sized&lt;true&gt;&gt; pool(128);

  // inference
  std::shared_ptr&lt;StaticRuntime&gt; runtime = nullptr;
  pool.pop(runtime);
  if (!runtime) {
    runtime = std::make_shared&lt;StaticRuntime&gt;(mod, opts);
  }
  auto output = runtime-&gt;run(args, kwargs);
  pool.push(runtime);</code></pre>
<h2 id="planned-features">Planned features</h2>
<ul>
<li>Memory planning</li>
<li>Operator dispatch inlining</li>
<li>Operator subsitution</li>
<li>Weight layout transformations (pre-packing)</li>
<li>Lowering to <code>torch.jit.tensorexpr</code></li>
</ul>
</body>
</html>
