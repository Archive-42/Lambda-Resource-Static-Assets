<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>GLOSSARY</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
</head>
<body>
<h1 id="pytorch-glossary">PyTorch Glossary</h1>
<ul>
<li><a href="#pytorch-glossary">PyTorch Glossary</a></li>
<li><a href="#operation-and-kernel">Operation and Kernel</a>
<ul>
<li><a href="#aten">ATen</a></li>
<li><a href="#operation">Operation</a></li>
<li><a href="#native-operation">Native Operation</a></li>
<li><a href="#custom-operation">Custom Operation</a></li>
<li><a href="#kernel">Kernel</a></li>
<li><a href="#compound-operation">Compound Operation</a></li>
<li><a href="#composite-operation">Composite Operation</a></li>
<li><a href="#non-leaf-operation">Non-Leaf Operation</a></li>
<li><a href="#leaf-operation">Leaf Operation</a></li>
<li><a href="#device-kernel">Device Kernel</a></li>
<li><a href="#compound-kernel">Compound Kernel</a></li>
</ul></li>
<li><a href="#jit-compilation">JIT Compilation</a>
<ul>
<li><a href="#jit">JIT</a></li>
<li><a href="#torchscript">TorchScript</a></li>
<li><a href="#tracing">Tracing</a></li>
<li><a href="#scripting">Scripting</a></li>
</ul></li>
</ul>
<h1 id="operation-and-kernel">Operation and Kernel</h1>
<h2 id="aten">ATen</h2>
<p>Short for “A Tensor Library”. The foundational tensor and mathematical operation library on which all else is built.</p>
<h2 id="operation">Operation</h2>
<p>A unit of work. For example, the work of matrix multiplication is an operation called aten::matmul.</p>
<h2 id="native-operation">Native Operation</h2>
<p>An operation that comes natively with PyTorch ATen, for example aten::matmul.</p>
<h2 id="custom-operation">Custom Operation</h2>
<p>An Operation that is defined by users and is usually a Compound Operation. For example, this <a href="https://pytorch.org/docs/stable/notes/extending.html">tutorial</a> details how to create Custom Operations.</p>
<h2 id="kernel">Kernel</h2>
<p>Implementation of a PyTorch operation, specifying what should be done when an operation executes.</p>
<h2 id="compound-operation">Compound Operation</h2>
<p>A Compound Operation is composed of other operations. Its kernel is usually device-agnostic. Normally it doesn’t have its own derivative functions defined. Instead, AutoGrad automatically computes its derivative based on operations it uses.</p>
<h2 id="composite-operation">Composite Operation</h2>
<p>Same as Compound Operation.</p>
<h2 id="non-leaf-operation">Non-Leaf Operation</h2>
<p>Same as Compound Operation.</p>
<h2 id="leaf-operation">Leaf Operation</h2>
<p>An operation that’s considered a basic operation, as opposed to a Compound Operation. Leaf Operation always has dispatch functions defined, usually has a derivative function defined as well.</p>
<h2 id="device-kernel">Device Kernel</h2>
<p>Device-specific kernel of a leaf operation.</p>
<h2 id="compound-kernel">Compound Kernel</h2>
<p>Opposed to Device Kernels, Compound kernels are usually device-agnostic and belong to Compound Operations.</p>
<h1 id="jit-compilation">JIT Compilation</h1>
<h2 id="jit">JIT</h2>
<p>Just-In-Time Compilation.</p>
<h2 id="torchscript">TorchScript</h2>
<p>An interface to the TorchScript JIT compiler and interpreter.</p>
<h2 id="tracing">Tracing</h2>
<p>Using <code>torch.jit.trace</code> on a function to get an executable that can be optimized using just-in-time compilation.</p>
<h2 id="scripting">Scripting</h2>
<p>Using <code>torch.jit.script</code> on a function to inspect source code and compile it as TorchScript code.</p>
</body>
</html>
