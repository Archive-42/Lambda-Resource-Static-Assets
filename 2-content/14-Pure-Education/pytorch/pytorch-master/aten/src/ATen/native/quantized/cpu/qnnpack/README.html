<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>README</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
</head>
<body>
<h1 id="qnnpack">QNNPACK</h1>
<p>QNNPACK (Quantized Neural Networks PACKage) is a mobile-optimized library for low-precision high-performance neural network inference. QNNPACK provides implementation of common neural network operators on quantized 8-bit tensors.</p>
<p>QNNPACK is not intended to be directly used by machine learning researchers; instead it provides low-level performance primitives for high-level deep learning frameworks. As of today, QNNPACK is integrated in <a href="https://github.com/pytorch/pytorch">PyTorch 1.0</a> with Caffe2 graph representation.</p>
<h2 id="operator-coverage">Operator Coverage</h2>
<p>Currently implemented and planned for implementation operators are below:</p>
<ul>
<li>[x] 2D Convolution</li>
<li>[x] 2D Deconvolution</li>
<li>[x] Channel Shuffle</li>
<li>[x] Fully Connected</li>
<li>[ ] Locally Connected</li>
<li>[x] 2D Max Pooling</li>
<li>[x] 2D Average Pooling</li>
<li>[x] Global Average Pooling</li>
<li>[x] Sigmoid</li>
<li>[x] TanH</li>
<li>[x] Leaky ReLU</li>
<li>[x] Hardsigmoid</li>
<li>[x] Hardswish</li>
<li>[x] Clamp (can be used for ReLU, ReLU6 if it is not fused in another operator)</li>
<li>[x] SoftArgMax (aka SoftMax)</li>
<li>[ ] Group Normalization</li>
</ul>
<h2 id="building">Building</h2>
<p>QNNPACK provides standard CMake-based build scripts.</p>
<h3 id="native-compilation">Native compilation</h3>
<p>Users are recommended to use <code>scripts/build-local.sh</code> script to build QNNPACK for the host machine.</p>
<h3 id="cross-compilation-for-android">Cross-compilation for Android</h3>
<p>To cross-compile for Android, set <code>$ANDROID_NDK</code> environment variable (where <code>$ANDROID_NDK</code> is the path to Android NDK directory, e.g. <code>/opt/android-ndk-r15c</code>) and use one of the scripts from the table below:</p>
<table>
<thead>
<tr class="header">
<th>ABI</th>
<th>Build script</th>
<th>Restrictions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>armeabi-v7a</td>
<td><code>scripts/build-android-armv7.sh</code></td>
<td>Requires CPU with ARM NEON</td>
</tr>
<tr class="even">
<td>arm64-v8a</td>
<td><code>scripts/build-android-arm64.sh</code></td>
<td></td>
</tr>
<tr class="odd">
<td>x86</td>
<td><code>scripts/build-android-x86.sh</code></td>
<td></td>
</tr>
</tbody>
</table>
<p>Notes: - On <strong>armeabi-v7a</strong> <code>pytorch_qnnp_initialize</code> will fail with <code>pytorch_qnnp_status_unsupported_hardware</code> if the mobile CPU does not support ARM NEON. Don’t set <code>-DANDROID_ARM_NEON=1</code> for QNNPACK compilation as it can make <code>pytorch_qnnp_initialize</code> crash on CPUs without ARM NEON.</p>
<h3 id="cross-compilation-for-ios">Cross-compilation for iOS</h3>
<p>To cross-compile for iOS, clone <a href="https://github.com/leetal/ios-cmake">ios-cmake</a>, and set <code>$IOS_CMAKE_TOOLCHAIN_FILE</code> environment variable (where <code>$IOS_CMAKE_TOOLCHAIN_FILE</code> is the path to <code>ios.toolchain.cmake</code> file in <a href="https://github.com/leetal/ios-cmake">ios-cmake</a>), and use one of the scripts from the table below:</p>
<table>
<thead>
<tr class="header">
<th>Architecture</th>
<th>Build script</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>armv7</td>
<td><code>scripts/build-ios-armv7.sh</code></td>
<td>iPhone 3GS/4/4S</td>
</tr>
<tr class="even">
<td>armv7</td>
<td><code>scripts/build-ios-armv7s.sh</code></td>
<td>iPhone 5 and newer</td>
</tr>
<tr class="odd">
<td>arm64</td>
<td><code>scripts/build-ios-arm64.sh</code></td>
<td>iPhone 5S and newer</td>
</tr>
<tr class="even">
<td>arm64e</td>
<td><code>scripts/build-ios-arm64e.sh</code></td>
<td>iPhone XS/XR</td>
</tr>
<tr class="odd">
<td>i386</td>
<td><code>scripts/build-ios-i386.sh</code></td>
<td>iPhone Simulator (32-bit)</td>
</tr>
<tr class="even">
<td>x86_64</td>
<td><code>scripts/build-ios-x86_64.sh</code></td>
<td>iPhone Simulator (64-bit)</td>
</tr>
</tbody>
</table>
<h2 id="end-to-end-benchmarking">End-to-End Benchmarking</h2>
<p>Caffe2 backend of PyTorch 1.0 natively integrates QNNPACK, and provides a <a href="https://github.com/caffe2/models/tree/master/mobilenet_v2_quantized">pre-trained quantized MobileNet v2 model</a>. Below are instructions for benchmarking this model end-to-end with QNNPACK.</p>
<h3 id="raspberry-pi-2-or-3">Raspberry Pi 2 or 3</h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb1-1" title="1"><span class="co"># Clone PyTorch 1.0 repo</span></a>
<a class="sourceLine" id="cb1-2" title="2"><span class="fu">git</span> clone --recursive https://github.com/pytorch/pytorch.git</a>
<a class="sourceLine" id="cb1-3" title="3"><span class="bu">cd</span> pytorch</a>
<a class="sourceLine" id="cb1-4" title="4"></a>
<a class="sourceLine" id="cb1-5" title="5"><span class="co"># Optional: update QNNPACK submodule to latest revision</span></a>
<a class="sourceLine" id="cb1-6" title="6"><span class="fu">git</span> submodule update --remote third_party/QNNPACK</a>
<a class="sourceLine" id="cb1-7" title="7"></a>
<a class="sourceLine" id="cb1-8" title="8"><span class="co"># Build Caffe2 (including binaries) for the host system</span></a>
<a class="sourceLine" id="cb1-9" title="9"><span class="co"># Use only 1 thread for build to avoid out-of-memory failures</span></a>
<a class="sourceLine" id="cb1-10" title="10"><span class="va">MAX_JOBS=</span>1 <span class="ex">scripts/build_local.sh</span> -DBUILD_BINARY=ON -DBUILD_PYTHON=OFF \</a>
<a class="sourceLine" id="cb1-11" title="11">    -DUSE_OBSERVERS=OFF -DUSE_DISTRIBUTED=OFF</a>
<a class="sourceLine" id="cb1-12" title="12"></a>
<a class="sourceLine" id="cb1-13" title="13"><span class="co"># Download model weights</span></a>
<a class="sourceLine" id="cb1-14" title="14"><span class="fu">wget</span> https://s3.amazonaws.com/download.caffe2.ai/models/mobilenet_v2_1.0_224_quant/init_net.pb</a>
<a class="sourceLine" id="cb1-15" title="15"></a>
<a class="sourceLine" id="cb1-16" title="16"><span class="co"># Download model graph</span></a>
<a class="sourceLine" id="cb1-17" title="17"><span class="fu">wget</span> https://s3.amazonaws.com/download.caffe2.ai/models/mobilenet_v2_1.0_224_quant/predict_net.pb</a>
<a class="sourceLine" id="cb1-18" title="18"></a>
<a class="sourceLine" id="cb1-19" title="19"><span class="co"># Run speed benchmark with 50 warm-up iterations and 10 measurement iterations</span></a>
<a class="sourceLine" id="cb1-20" title="20"><span class="ex">build/bin/speed_benchmark</span> --net predict_net.pb --init_net init_net.pb \</a>
<a class="sourceLine" id="cb1-21" title="21">    --input data --input_dims 1,3,224,224 --input_type float \</a>
<a class="sourceLine" id="cb1-22" title="22">    --warmup 50 --iter 10</a></code></pre></div>
<h3 id="armv7-32-bit-android">ARMv7 (32-bit) Android</h3>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb2-1" title="1"><span class="co"># Clone PyTorch 1.0 repo</span></a>
<a class="sourceLine" id="cb2-2" title="2"><span class="fu">git</span> clone --recursive https://github.com/pytorch/pytorch.git</a>
<a class="sourceLine" id="cb2-3" title="3"><span class="bu">cd</span> pytorch</a>
<a class="sourceLine" id="cb2-4" title="4"></a>
<a class="sourceLine" id="cb2-5" title="5"><span class="co"># Optional: update QNNPACK submodule to latest revision</span></a>
<a class="sourceLine" id="cb2-6" title="6"><span class="fu">git</span> submodule update --remote third_party/QNNPACK</a>
<a class="sourceLine" id="cb2-7" title="7"></a>
<a class="sourceLine" id="cb2-8" title="8"><span class="co"># Build Caffe2 (including binaries) for Android, and push to device</span></a>
<a class="sourceLine" id="cb2-9" title="9"><span class="ex">scripts/build_android.sh</span> -DANDROID_TOOLCHAIN=clang -DBUILD_BINARY=ON</a>
<a class="sourceLine" id="cb2-10" title="10"><span class="ex">adb</span> push build_android/bin/speed_benchmark /data/local/tmp/speed_benchmark</a>
<a class="sourceLine" id="cb2-11" title="11"></a>
<a class="sourceLine" id="cb2-12" title="12"><span class="co"># Download model weights and copy them to Android device</span></a>
<a class="sourceLine" id="cb2-13" title="13"><span class="fu">wget</span> https://s3.amazonaws.com/download.caffe2.ai/models/mobilenet_v2_1.0_224_quant/init_net.pb</a>
<a class="sourceLine" id="cb2-14" title="14"><span class="ex">adb</span> push init_net.pb /data/local/tmp/init_net.pb</a>
<a class="sourceLine" id="cb2-15" title="15"></a>
<a class="sourceLine" id="cb2-16" title="16"><span class="co"># Download model graph and copy it to Android device</span></a>
<a class="sourceLine" id="cb2-17" title="17"><span class="fu">wget</span> https://s3.amazonaws.com/download.caffe2.ai/models/mobilenet_v2_1.0_224_quant/predict_net.pb</a>
<a class="sourceLine" id="cb2-18" title="18"><span class="ex">adb</span> push predict_net.pb /data/local/tmp/predict_net.pb</a>
<a class="sourceLine" id="cb2-19" title="19"></a>
<a class="sourceLine" id="cb2-20" title="20"><span class="co"># Run speed benchmark with 50 warm-up iterations and 10 measurement iterations</span></a>
<a class="sourceLine" id="cb2-21" title="21"><span class="ex">adb</span> shell /data/local/tmp/speed_benchmark \</a>
<a class="sourceLine" id="cb2-22" title="22">    --net /data/local/tmp/predict_net.pb \</a>
<a class="sourceLine" id="cb2-23" title="23">    --init_net /data/local/tmp/init_net.pb \</a>
<a class="sourceLine" id="cb2-24" title="24">    --input data --input_dims 1,3,224,224 --input_type float \</a>
<a class="sourceLine" id="cb2-25" title="25">    --warmup 50 --iter 10</a></code></pre></div>
<h3 id="arm64-64-bit-android">ARM64 (64-bit) Android</h3>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb3-1" title="1"><span class="co"># Clone PyTorch 1.0 repo</span></a>
<a class="sourceLine" id="cb3-2" title="2"><span class="fu">git</span> clone --recursive https://github.com/pytorch/pytorch.git</a>
<a class="sourceLine" id="cb3-3" title="3"><span class="bu">cd</span> pytorch</a>
<a class="sourceLine" id="cb3-4" title="4"></a>
<a class="sourceLine" id="cb3-5" title="5"><span class="co"># Optional: update QNNPACK submodule to latest revision</span></a>
<a class="sourceLine" id="cb3-6" title="6"><span class="fu">git</span> submodule update --remote third_party/QNNPACK</a>
<a class="sourceLine" id="cb3-7" title="7"></a>
<a class="sourceLine" id="cb3-8" title="8"><span class="co"># Build Caffe2 (including binaries) for Android, and push to device</span></a>
<a class="sourceLine" id="cb3-9" title="9"><span class="ex">scripts/build_android.sh</span> -DANDROID_ABI=arm64-v8a -DANDROID_TOOLCHAIN=clang -DBUILD_BINARY=ON</a>
<a class="sourceLine" id="cb3-10" title="10"><span class="ex">adb</span> push build_android/bin/speed_benchmark /data/local/tmp/speed_benchmark</a>
<a class="sourceLine" id="cb3-11" title="11"></a>
<a class="sourceLine" id="cb3-12" title="12"><span class="co"># Download model weights and copy them to Android device</span></a>
<a class="sourceLine" id="cb3-13" title="13"><span class="fu">wget</span> https://s3.amazonaws.com/download.caffe2.ai/models/mobilenet_v2_1.0_224_quant/init_net.pb</a>
<a class="sourceLine" id="cb3-14" title="14"><span class="ex">adb</span> push init_net.pb /data/local/tmp/init_net.pb</a>
<a class="sourceLine" id="cb3-15" title="15"></a>
<a class="sourceLine" id="cb3-16" title="16"><span class="co"># Download model graph and copy it to Android device</span></a>
<a class="sourceLine" id="cb3-17" title="17"><span class="fu">wget</span> https://s3.amazonaws.com/download.caffe2.ai/models/mobilenet_v2_1.0_224_quant/predict_net.pb</a>
<a class="sourceLine" id="cb3-18" title="18"><span class="ex">adb</span> push predict_net.pb /data/local/tmp/predict_net.pb</a>
<a class="sourceLine" id="cb3-19" title="19"></a>
<a class="sourceLine" id="cb3-20" title="20"><span class="co"># Run speed benchmark with 50 warm-up iterations and 10 measurement iterations</span></a>
<a class="sourceLine" id="cb3-21" title="21"><span class="ex">adb</span> shell /data/local/tmp/speed_benchmark \</a>
<a class="sourceLine" id="cb3-22" title="22">    --net /data/local/tmp/predict_net.pb \</a>
<a class="sourceLine" id="cb3-23" title="23">    --init_net /data/local/tmp/init_net.pb \</a>
<a class="sourceLine" id="cb3-24" title="24">    --input data --input_dims 1,3,224,224 --input_type float \</a>
<a class="sourceLine" id="cb3-25" title="25">    --warmup 50 --iter 10</a></code></pre></div>
<h3 id="pep-performance-evaluation-platform-method">PEP (Performance Evaluation Platform) Method</h3>
<p><a href="https://github.com/facebook/FAI-PEP">Facebook AI Performance Evaluation Platform</a> is a framework and backend agnostic benchmarking platform to compare machine learning inferencing runtime metrics on a set of models and a variety of backends.</p>
<p>We use PEP to produce the results we have in our <a href="https://code.fb.com/ml-applications/qnnpack/">blog</a></p>
<p>With an ARMv7 device connected:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb4-1" title="1"><span class="co"># Clone PyTorch 1.0 repo</span></a>
<a class="sourceLine" id="cb4-2" title="2"><span class="fu">mkdir</span> ~/Code <span class="kw">&amp;&amp;</span> <span class="bu">cd</span> ~/Code</a>
<a class="sourceLine" id="cb4-3" title="3"><span class="fu">git</span> clone --recursive https://github.com/pytorch/pytorch.git</a>
<a class="sourceLine" id="cb4-4" title="4"><span class="bu">cd</span> pytorch</a>
<a class="sourceLine" id="cb4-5" title="5"></a>
<a class="sourceLine" id="cb4-6" title="6"><span class="co"># Optional: update QNNPACK submodule to latest revision</span></a>
<a class="sourceLine" id="cb4-7" title="7"><span class="fu">git</span> submodule update --remote third_party/QNNPACK</a>
<a class="sourceLine" id="cb4-8" title="8"></a>
<a class="sourceLine" id="cb4-9" title="9"><span class="co"># Clone PEP repo</span></a>
<a class="sourceLine" id="cb4-10" title="10"><span class="bu">cd</span> ~/Code</a>
<a class="sourceLine" id="cb4-11" title="11"><span class="fu">git</span> clone --recursive https://github.com/facebook/FAI-PEP.git aibench</a>
<a class="sourceLine" id="cb4-12" title="12"><span class="bu">cd</span> aibench</a>
<a class="sourceLine" id="cb4-13" title="13"></a>
<a class="sourceLine" id="cb4-14" title="14"><span class="co"># Run PEP benchmark with cool specifications. Try changing that cmd with more specifications!</span></a>
<a class="sourceLine" id="cb4-15" title="15"><span class="co"># First time compile could take 20+ minutes</span></a>
<a class="sourceLine" id="cb4-16" title="16"><span class="ex">./benchmarking/run_bench.py</span> \</a>
<a class="sourceLine" id="cb4-17" title="17">  --platform android \</a>
<a class="sourceLine" id="cb4-18" title="18">  -b ~/Code/aibench/specifications/models/caffe2/mobilenet_v2/mobilenet_v2_quant.json \</a>
<a class="sourceLine" id="cb4-19" title="19">  --platform android --repo_dir ~/Code/pytorch \</a>
<a class="sourceLine" id="cb4-20" title="20">  --frameworks_dir ~/Code/aibench/specifications/frameworks --framework caffe2</a></code></pre></div>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>QNNPACK is developed by Marat Dukhan, Yiming Wu, Hao Lu, and Bert Maher. We thank Andrew Tulloch and Yangqing Jia for advice during the development of QNNPACK.</p>
<h2 id="license">License</h2>
<p>QNNPACK is BSD licensed, as found in the <a href="LICENSE"><code>LICENSE</code></a> file.</p>
</body>
</html>
