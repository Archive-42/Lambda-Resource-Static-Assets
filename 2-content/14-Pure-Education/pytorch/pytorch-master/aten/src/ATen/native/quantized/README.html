<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>README</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
</head>
<body>
<p>The quantized folder holds the implementation of the low-level quantized kernel. The kernels are registered in <code>torch::_ops</code> namespace, and operate on the quantized <code>at::Tensor</code> data type. You can learn more about the quantized tensors in the <a href="https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor">quantized tensor API wiki</a> page.</p>
<p>This document serves as an entry point for quantized kernel implementation.</p>
<h2 id="implementing-native-quantized-ops">Implementing native quantized ops</h2>
<p>The new quantized ops are almost always located under the <code>ATen/native/quantized/cpu</code> folder. For the sake of an example, let us implement an element-wise quantized <a href="https://en.wiktionary.org/wiki/XAND">logical XAND</a> operation under <code>ATen/native/quantized/cpu/qxand.cpp</code>.</p>
<h3 id="step-0.-implement-the-quantized-function">Step 0. Implement the quantized function</h3>
<p>Before writing the quantized kernel and registering it, let us implement a quantized function. That would assist in any further discussion. The snippet below shows the implementation of a quantized XAND operator, with the support of all implemented quantized types.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode cpp"><code class="sourceCode cpp"><a class="sourceLine" id="cb1-1" title="1">Tensor quantized_xand(Tensor qa, Tensor qb) {</a>
<a class="sourceLine" id="cb1-2" title="2">  <span class="co">// Some type checks for qa and qb should be here...</span></a>
<a class="sourceLine" id="cb1-3" title="3">  Tensor qc;</a>
<a class="sourceLine" id="cb1-4" title="4">  <span class="dt">double</span> scale = qa.q_scale();</a>
<a class="sourceLine" id="cb1-5" title="5">  <span class="dt">int64_t</span> zero_point = qa.q_zero_point();</a>
<a class="sourceLine" id="cb1-6" title="6"></a>
<a class="sourceLine" id="cb1-7" title="7">  <span class="kw">auto</span> iter = TensorIterator::binary_op(qc, qa, qb);</a>
<a class="sourceLine" id="cb1-8" title="8"></a>
<a class="sourceLine" id="cb1-9" title="9">  AT_DISPATCH_QINT_TYPES(qa.<span class="dt">scalar_type</span>(), <span class="st">&quot;quantized_xand&quot;</span>, [&amp;]() {</a>
<a class="sourceLine" id="cb1-10" title="10">    Tensor qc = at::_empty_affine_quantized(</a>
<a class="sourceLine" id="cb1-11" title="11">        qa.sizes(), at::device(kCPU).dtype(SCALAR_TYPE), scale, zero_point);</a>
<a class="sourceLine" id="cb1-12" title="12">    cpu_kernel(iter, [&amp;](<span class="dt">scalar_t</span> a_value, <span class="dt">scalar_t</span> b_value) -&gt; <span class="dt">scalar_t</span> {</a>
<a class="sourceLine" id="cb1-13" title="13">      <span class="cf">return</span> <span class="dt">scalar_t</span>(a_value.<span class="va">val_</span> &amp; b_value.<span class="va">val_</span>);</a>
<a class="sourceLine" id="cb1-14" title="14">    });</a>
<a class="sourceLine" id="cb1-15" title="15">  });</a>
<a class="sourceLine" id="cb1-16" title="16">  <span class="cf">return</span> qc;</a>
<a class="sourceLine" id="cb1-17" title="17">}</a></code></pre></div>
<p>The code above is fairly straight-forward: It takes two quantized tensors <code>qa</code> and <code>qb</code>, and uses <code>binary_kernel</code> to produce a quantized tensor <code>qc</code>. We also use the <a href="https://caffe2.ai/doxygen-c/html/structat_1_1_tensor_iterator.html"><code>TensorIterator</code></a> in this example. The only part that that requires explicit explanation is the <code>AT_DISPATCH_QINT_TYPES</code>. This macro makes sure that the underlying code works with all quantized types. It provides several useful “aliases”:</p>
<ul>
<li><code>SCALAR_TYPE</code> – <code>ScalarType</code> of the quantized tensor (e.g. <code>kQInt8</code>)</li>
<li><code>scalar_t</code> – quantized data type (dtype, e.g. <code>qint8</code>)</li>
<li><code>underlying_t</code> – underlying POD data type (dtype, e.g. <code>int8_t</code>)</li>
</ul>
<p>The macro takes three arguments:</p>
<ol type="1">
<li>Quantized data type. This will define what the “aliases” are. In the example above, the resulting tensor will be the same as the <code>qa.scalar_type()</code>.</li>
<li>Function name. This argument is currently used for error reporting.</li>
<li>Implementation lambda. The main implementation should sit in the body of this lambda. it should also use the aliases for the quantized data types instead of the explicit data types.</li>
</ol>
<h3 id="step-1.-define-the-schema">Step 1. Define the schema</h3>
<p>Update <code>aten/src/ATen/native/quantized/library.cpp</code> and add a <code>def</code> for your new operator:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode cpp"><code class="sourceCode cpp"><a class="sourceLine" id="cb2-1" title="1">TORCH_LIBRARY(quantized, m) {</a>
<a class="sourceLine" id="cb2-2" title="2">  <span class="co">// ... the existing definitions ...</span></a>
<a class="sourceLine" id="cb2-3" title="3">  m.def(<span class="st">&quot;quantized::xand(Tensor qa, Tensor qb) -&gt; Tensor&quot;</span>);</a>
<a class="sourceLine" id="cb2-4" title="4">}</a></code></pre></div>
<p>Def takes a <strong>function schema string</strong>: This schema describes the usage of the op. In the example above the schema is <code>"quantized::xand(Tensor qa, Tensor qb) -&gt; Tensor"</code>. This translates to <code>torch._ops.ops.quantized.xand</code> function in Python of the appropriate signature.</p>
<h3 id="step-2.-register-the-implementation">Step 2. Register the implementation</h3>
<p>The registration is done using <code>TORCH_LIBRARY_IMPL</code>.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode cpp"><code class="sourceCode cpp"><a class="sourceLine" id="cb3-1" title="1">TORCH_LIBRARY_IMPL(quantized, QuantizedCPU, m) {</a>
<a class="sourceLine" id="cb3-2" title="2">  m.impl(<span class="st">&quot;xand&quot;</span>, TORCH_FN(quantized_xand));</a>
<a class="sourceLine" id="cb3-3" title="3">}</a></code></pre></div>
<h3 id="step-2b.-optional-registering-the-operation-with-the-native_functions.yaml">Step 2b. [Optional] Registering the operation with the <code>native_functions.yaml</code></h3>
<p>In some cases, if the signature of the quantized function and its non-quantized counterpart are the same, it is worth adding it to the <code>ATen/native/native_functions.yaml</code>. A detailed explanation on this file can be found <a href="https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md">here</a>.</p>
<p><strong>If adding a new entry to the <code>native_functions.yaml</code>:</strong></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode yaml"><code class="sourceCode yaml"><a class="sourceLine" id="cb4-1" title="1"><span class="kw">-</span> <span class="fu">func:</span><span class="at"> quantized_xand(Tensor qa, Tensor qb) -&gt; Tensor</span></a>
<a class="sourceLine" id="cb4-2" title="2">  <span class="fu">dispatch:</span></a>
<a class="sourceLine" id="cb4-3" title="3">    <span class="fu">QuantizedCPU:</span><span class="at"> quantized_xand</span></a></code></pre></div>
<p><strong>If adding to an existing entry in the <code>native_functions.yaml</code>:</strong></p>
<p>If you find an entry in the yaml file, and would like to add a quantized kernel to it, you can just add a new dispatch entry for it. For example, let’s assume there existed a <code>xand</code> function in the YAML file. In that case, modification would look as:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode yaml"><code class="sourceCode yaml"><a class="sourceLine" id="cb5-1" title="1"><span class="kw">-</span> <span class="fu">func:</span><span class="at"> xand(Tensor a, Tensor b) -&gt; Tensor</span></a>
<a class="sourceLine" id="cb5-2" title="2">  <span class="fu">dispatch:</span></a>
<a class="sourceLine" id="cb5-3" title="3">    <span class="fu">CPU:</span><span class="at"> _xand_cpu     </span><span class="co"># Assume this existed</span></a>
<a class="sourceLine" id="cb5-4" title="4">    <span class="fu">CUDA:</span><span class="at"> _xand_cuda   </span><span class="co"># Assume this existed</span></a>
<a class="sourceLine" id="cb5-5" title="5">    <span class="fu">QuantizedCPU:</span><span class="at"> quantized_xand</span></a></code></pre></div>
<h3 id="putting-it-all-together">Putting it all together</h3>
<p>The final file <code>ATen/native/quantized/cpu/qxand.cpp</code> would look as follows</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode cpp"><code class="sourceCode cpp"><a class="sourceLine" id="cb6-1" title="1"><span class="pp">#include </span><span class="im">&lt;ATen/ATen.h&gt;</span></a>
<a class="sourceLine" id="cb6-2" title="2"><span class="pp">#include </span><span class="im">&lt;ATen/NativeFunctions.h&gt;</span><span class="pp"> </span><span class="co">// Need that for the `native_functions.yaml`</span></a>
<a class="sourceLine" id="cb6-3" title="3"><span class="pp">#include </span><span class="im">&lt;ATen/core/Type.h&gt;</span></a>
<a class="sourceLine" id="cb6-4" title="4"><span class="pp">#include </span><span class="im">&lt;torch/library.h&gt;</span></a>
<a class="sourceLine" id="cb6-5" title="5"><span class="pp">#include </span><span class="im">&lt;ATen/native/TensorIterator.h&gt;</span></a>
<a class="sourceLine" id="cb6-6" title="6"><span class="pp">#include </span><span class="im">&lt;ATen/native/cpu/Loops.h&gt;</span></a>
<a class="sourceLine" id="cb6-7" title="7"></a>
<a class="sourceLine" id="cb6-8" title="8"><span class="kw">namespace</span> at {</a>
<a class="sourceLine" id="cb6-9" title="9">  <span class="kw">namespace</span> native {</a>
<a class="sourceLine" id="cb6-10" title="10">  Tensor quantized_xand(Tensor qa, Tensor qb) {</a>
<a class="sourceLine" id="cb6-11" title="11">    <span class="co">// The awesome op implementation...</span></a>
<a class="sourceLine" id="cb6-12" title="12">    <span class="cf">return</span> qc;</a>
<a class="sourceLine" id="cb6-13" title="13">  }</a>
<a class="sourceLine" id="cb6-14" title="14"></a>
<a class="sourceLine" id="cb6-15" title="15">  TORCH_LIBRARY_IMPL(quantized, QuantizedCPU, m) {</a>
<a class="sourceLine" id="cb6-16" title="16">    m.impl(<span class="st">&quot;xand&quot;</span>, TORCH_FN(quantized_xand));</a>
<a class="sourceLine" id="cb6-17" title="17">  }</a>
<a class="sourceLine" id="cb6-18" title="18">}}  <span class="co">// namespace at::native</span></a></code></pre></div>
<h3 id="step-3.-administrative-stuff">Step 3. Administrative stuff</h3>
<p>Before the op can be used, it needs to be compiled. If the op is placed under <code>native/quantized/cpu</code>, this already done for you. However, if the location is changed, two files must be notified:</p>
<ul>
<li><em><code>caffe2/aten/TARGETS</code></em> – You can follow the same example, and add your path in somewhere in that file. Notice in this file we places the path to the quantized source files:</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb7-1" title="1"><span class="ex">ATEN_NATIVE_CPP</span> = glob([</a>
<a class="sourceLine" id="cb7-2" title="2"><span class="co">#...</span></a>
<a class="sourceLine" id="cb7-3" title="3">  <span class="st">&quot;src/ATen/native/quantized/**/*.cpp&quot;</span>,</a>
<a class="sourceLine" id="cb7-4" title="4">])</a></code></pre></div>
<ul>
<li><em><code>caffe2/aten/src/ATen/CMakeLists.txt</code></em> – Again, following the example, you must add your paths. The current quantization paths are added as</li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb8-1" title="1"><span class="ex">FILE</span>(GLOB native_quantized_cpp</a>
<a class="sourceLine" id="cb8-2" title="2">          <span class="st">&quot;native/quantized/*.cpp&quot;</span></a>
<a class="sourceLine" id="cb8-3" title="3">          <span class="st">&quot;native/quantized/cpu/*.cpp&quot;</span>)</a></code></pre></div>
<h2 id="using-quantized-ops">Using quantized ops</h2>
<h3 id="python">Python</h3>
<p>Usage in Python is pretty easy. To implement the python quantized function using our kernel, you can do the following</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" title="1"><span class="im">from</span> torch._ops <span class="im">import</span> ops</a>
<a class="sourceLine" id="cb9-2" title="2"></a>
<a class="sourceLine" id="cb9-3" title="3"><span class="kw">def</span> quantized_xand(qa, qb):</a>
<a class="sourceLine" id="cb9-4" title="4"><span class="co">#Notice the schema changed from `quantized::xand` to `quantized.xand`</span></a>
<a class="sourceLine" id="cb9-5" title="5">  <span class="cf">return</span> ops.quantized.xand(qa, qb)</a></code></pre></div>
<p><strong>Note:</strong> If writing new pytorch functions that use quantized kernels, it is strongly encouraged to place them in the <code>torch/nn/quantized/functional.py</code>.</p>
<h3 id="c">C++</h3>
<p>You should not need to use the registered kernels in C++. Although <strong>officially not supported</strong>, you can use the following</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode cpp"><code class="sourceCode cpp"><a class="sourceLine" id="cb10-1" title="1">  Tensor quantized_xand(Tensor qa, Tensor qb) {</a>
<a class="sourceLine" id="cb10-2" title="2">    <span class="at">static</span> <span class="at">const</span> c10::OperatorHandle op = c10::Dispatcher::singleton().findSchema({<span class="st">&quot;quantized::xand&quot;</span>, <span class="st">&quot;&quot;</span>}).value();</a>
<a class="sourceLine" id="cb10-3" title="3">    <span class="cf">return</span> op.call&lt;Tensor, Tensor, Tensor&gt;(qa, qb);</a>
<a class="sourceLine" id="cb10-4" title="4">  }</a></code></pre></div>
</body>
</html>
